// MCP Server - Main entry point for MCP protocol
import { scan } from '../scanner/index.js';
import { route } from '../router/index.js';
import { orchestrate } from '../orchestrator/index.js';
import { memory } from '../memory/index.js';
import { fileEngine } from '../file-engine/index.js';
import { generateProposals, validateProposal } from '../proposals/index.js';
import { initLLM, isConfigured, chatWithSystem, loadConfig } from '../llm/index.js';
import { getSystemPrompt, getUserPrompt, validateBudget, TOKEN_BUDGET, OUTPUT_FORMAT } from '../llm/prompts.js';

/**
 * In-memory store for pending proposals
 * In production, this should be persisted
 */
const pendingProposals = new Map();

/**
 * Generate proposals using LLM (when available)
 * OPTIMIZED: Minimal tokens, compressed metadata
 */
async function generateLLMProposals(projectPath, userIntent, metadata) {
  if (!isConfigured()) {
    return null; // Fall back to deterministic
  }
  
  // OPTIMIZED: Compact prompts
  const systemPrompt = getSystemPrompt(metadata);
  const userPrompt = getUserPrompt(userIntent, metadata);
  
  // Token budget validation
  const budget = validateBudget(systemPrompt, userPrompt);
  if (!budget.valid) {
    console.error('[MCP] Token budget exceeded:', budget);
  }
  
  try {
    const response = await chatWithSystem(systemPrompt, userPrompt, {
      maxTokens: TOKEN_BUDGET.MAX_RESPONSE_TOKENS
    });
    
    if (!response.success) {
      console.error('[MCP] LLM error:', response.error);
      return null;
    }
    
    // Parse JSON response
    let changes;
    try {
      changes = JSON.parse(response.content);
    } catch {
      // Try to extract JSON from response
      const jsonMatch = response.content.match(/\[[\s\S]*\]/);
      if (jsonMatch) {
        changes = JSON.parse(jsonMatch[0]);
      } else {
        return null;
      }
    }
    
    // Handle compact format: {f, t, c} -> {file, type, content}
    const normalizedChanges = changes.map((change, i) => ({
      file: change.f || change.file,
      type: change.t || change.type,
      content: change.c || change.content
    })).filter(c => c.file && c.type);
    
    // Convert to proposals format
    return normalizedChanges.map((change, i) => ({
      id: 'llm-' + Date.now() + '-' + i,
      agent: 'llm',
      description: 'Generated by LLM: ' + change.file,
      change: {
        type: change.type === 'c' ? 'create' : change.type === 'u' ? 'update' : change.type === 'd' ? 'delete' : change.type,
        file: change.file,
        content: change.content || ''
      },
      originalContent: '',
      risks: ['LLM-generated code - review before applying'],
      llm: true,
      tokenUsage: budget.total
    }));
  } catch (e) {
    console.error('[MCP] LLM parsing error:', e.message);
    return null;
  }
}

/**
 * Main analyze function - MCP interface
 * Now includes proposal generation
 * @param {Object} params 
 * @param {string} params.projectPath
 * @param {string} params.userIntent
 * @param {boolean} params.generateProposals - Whether to generate proposals (default: true)
 * @returns {Object} MCPOutput
 */
export async function analyze({ projectPath, userIntent, generateProposals: doGenerateProposals = true }) {
  try {
    // Step 1: Scan project
    console.error('[MCP] Scanning project:', projectPath);
    const metadata = scan(projectPath);
    console.error('[MCP] Metadata:', JSON.stringify(metadata));
    
    // Step 2: Route to agents
    console.error('[MCP] Routing agents for intent:', userIntent);
    const { agents: plan, reason } = await route({ metadata, userIntent });
    console.error('[MCP] Selected agents:', plan.map(a => a.agentId).join(', '));
    console.error('[MCP] Reason:', reason);
    
    // Step 3: Orchestrate execution
    console.error('[MCP] Executing agents...');
    const { results, summary } = await orchestrate({
      projectPath,
      metadata,
      plan,
      userIntent
    });
    
    // Step 4: Collect diagnostics and changes
    const allDiagnostics = results.flatMap(r => r.diagnostics || []);
    const allChanges = results.flatMap(r => r.changes || []);
    
    // Step 5: Generate proposals if requested
    let proposals = [];
    if (doGenerateProposals) {
      console.error('[MCP] Generating proposals...');
      
      // Try LLM first if configured
      if (isConfigured()) {
        console.error('[MCP] Using LLM for intelligent proposals...');
        const llmProposals = await generateLLMProposals(projectPath, userIntent, metadata);
        if (llmProposals && llmProposals.length > 0) {
          proposals = llmProposals;
        }
      }
      
      // Fall back to deterministic if no LLM or no results
      if (proposals.length === 0) {
        console.error('[MCP] Using deterministic proposals...');
        const proposalResult = await generateProposals(projectPath, userIntent, metadata);
        proposals = proposalResult.proposals || [];
      }
      
      // Store proposals for later apply
      if (proposals.length > 0) {
        const proposalKey = projectPath + ':' + Date.now();
        pendingProposals.set(proposalKey, { projectPath, proposals });
      }
    }
    
    // Step 6: Save to memory
    const agentIds = plan.map(p => p.agentId);
    const success = results.some(r => r.success);
    const runSummary = results.map(r => r.summary).join('; ');
    
    memory.saveRun(
      projectPath,
      agentIds,
      userIntent,
      success,
      runSummary
    );
    
    const memoryReference = memory.getReference(projectPath);
    
    // Return structured response
    return {
      summary: summary + ' ' + runSummary,
      diagnostics: allDiagnostics,
      changes: allChanges,
      proposals: proposals.map(p => ({
        id: p.id,
        agent: p.agent,
        description: p.description,
        file: p.change?.file,
        type: p.change?.type,
        diff: p.diff?.diff || '',
        additions: p.diff?.additions || 0,
        deletions: p.diff?.deletions || 0,
        risks: p.risks || []
      })),
      memoryReference
    };
    
  } catch (error) {
    console.error('[MCP] Error:', error.message);
    return {
      summary: 'Error: ' + error.message,
      diagnostics: [{
        severity: 'error',
        message: error.message,
        file: '',
        line: 0
      }],
      changes: [],
      proposals: [],
      memoryReference: ''
    };
  }
}

/**
 * Preview a proposal (dry-run)
 */
export async function previewProposal({ projectPath, proposalId }) {
  // Find proposal in pending
  let foundProposal = null;
  let proposalKey = null;
  
  for (const [key, data] of pendingProposals) {
    if (data.projectPath === projectPath) {
      foundProposal = data.proposals.find(p => p.id === proposalId);
      proposalKey = key;
      break;
    }
  }
  
  if (!foundProposal) {
    return {
      success: false,
      error: 'Proposal not found. Run analyze first to generate proposals.'
    };
  }
  
  // Validate
  const validation = validateProposal(foundProposal);
  if (!validation.valid) {
    return {
      success: false,
      errors: validation.errors,
      warnings: validation.warnings
    };
  }
  
  // Dry-run apply
  const preview = fileEngine.previewChanges(projectPath, [foundProposal.change]);
  
  return {
    success: true,
    proposal: {
      id: foundProposal.id,
      description: foundProposal.description,
      file: foundProposal.change?.file,
      type: foundProposal.change?.type,
      diff: foundProposal.diff?.diff || ''
    },
    preview: preview[0],
    validation: {
      valid: true,
      warnings: validation.warnings
    }
  };
}

/**
 * Apply a proposal
 */
export async function applyProposal({ projectPath, proposalId, backup = true }) {
  // Find proposal
  let foundProposal = null;
  
  for (const [key, data] of pendingProposals) {
    if (data.projectPath === projectPath) {
      foundProposal = data.proposals.find(p => p.id === proposalId);
      break;
    }
  }
  
  if (!foundProposal) {
    return {
      success: false,
      error: 'Proposal not found. Run analyze first to generate proposals.'
    };
  }
  
  // Validate first
  const validation = validateProposal(foundProposal);
  if (!validation.valid) {
    return {
      success: false,
      errors: validation.errors,
      warnings: validation.warnings
    };
  }
  
  // Apply with backup
  const applyFn = backup ? fileEngine.applyWithBackup : fileEngine.applyWithoutBackup;
  const result = applyFn(projectPath, [foundProposal.change]);
  
  return {
    success: result[0]?.success || false,
    applied: result[0]?.success ? [{
      file: foundProposal.change?.file,
      action: result[0]?.action,
      backupPath: result[0]?.backupPath || ''
    }] : [],
    errors: result[0]?.success ? [] : [result[0]?.error],
    warnings: validation.warnings
  };
}

/**
 * Apply all pending proposals
 */
export async function applyAllProposals({ projectPath, backup = true }) {
  let allProposals = [];
  
  for (const [key, data] of pendingProposals) {
    if (data.projectPath === projectPath) {
      allProposals = data.proposals;
      break;
    }
  }
  
  if (allProposals.length === 0) {
    return {
      success: false,
      error: 'No proposals found. Run analyze first.'
    };
  }
  
  // Apply all
  const applyFn = backup ? fileEngine.applyWithBackup : fileEngine.applyWithoutBackup;
  const result = applyFn(projectPath, allProposals.map(p => p.change));
  
  return {
    success: result.every(r => r.success),
    applied: result.map((r, i) => ({
      file: allProposals[i].change?.file,
      action: r.action,
      backupPath: r.backupPath || ''
    })),
    errors: result.filter(r => !r.success).map(r => r.error)
  };
}

/**
 * Get memory status for a project
 */
export async function getMemoryStatus({ projectPath }) {
  try {
    return memory.getStatus(projectPath);
  } catch (error) {
    return {
      error: error.message
    };
  }
}

/**
 * Get memory configuration
 */
export function getMemoryConfig() {
  return memory.getConfig();
}

/**
 * Get LLM status
 */
export function getLLMStatus() {
  if (!isConfigured()) {
    return {
      configured: false,
      message: 'LLM not configured. Set environment variables: LLM_PROVIDER, LLM_API_KEY, LLM_MODEL'
    };
  }
  
  const config = getConfig();
  return {
    configured: true,
    provider: config.provider,
    model: config.defaultModel,
    baseUrl: config.baseUrl || 'default'
  };
}

// CLI interface
export async function runCLI() {
  // Try to load LLM config from environment
  const llmConfig = loadConfig();
  if (llmConfig) {
    console.error('[CLI] LLM loaded:', llmConfig.provider, '-', llmConfig.defaultModel);
  }
  
  const args = process.argv.slice(2);
  let projectPath = '.';
  let userIntent = '';
  let command = 'analyze';
  let proposalId = '';
  
  // Parse arguments
  for (let i = 0; i < args.length; i++) {
    if (args[i] === '--project' && args[i + 1]) {
      projectPath = args[i + 1];
      i++;
    } else if (args[i] === '--prompt' && args[i + 1]) {
      userIntent = args[i + 1];
      i++;
    } else if (args[i] === '--status') {
      command = 'status';
    } else if (args[i] === '--config') {
      command = 'config';
    } else if (args[i] === '--apply' && args[i + 1]) {
      command = 'apply';
      proposalId = args[i + 1];
      i++;
    } else if (args[i] === '--preview' && args[i + 1]) {
      command = 'preview';
      proposalId = args[i + 1];
      i++;
    } else if (args[i] === '--no-backup') {
      // Flag for apply commands
    }
  }
  
  if (command === 'analyze' && !userIntent) {
    console.log('Usage: node index.js --project <path> --prompt "<intent>"');
    console.log('       node index.js --project <path> --status');
    console.log('       node index.js --project <path> --config');
    console.log('       node index.js --project <path> --preview <proposal-id>');
    console.log('       node index.js --project <path> --apply <proposal-id>');
    process.exit(1);
  }
  
  console.error('=== ai-core ===');
  
  if (command === 'analyze') {
    console.error('Project:', projectPath);
    console.error('Intent:', userIntent);
    console.error('');
    
    const result = await analyze({ projectPath, userIntent });
    console.log(JSON.stringify(result, null, 2));
    
    if (result.proposals?.length > 0) {
      console.error('');
      console.error('Proposals generated. Use --preview or --apply to act on them.');
    }
  } else if (command === 'status') {
    console.error('Project:', projectPath);
    console.error('');
    
    const status = await getMemoryStatus({ projectPath });
    console.log(JSON.stringify(status, null, 2));
  } else if (command === 'config') {
    const config = getMemoryConfig();
    console.log(JSON.stringify(config, null, 2));
  } else if (command === 'preview') {
    console.error('Project:', projectPath);
    console.error('Proposal:', proposalId);
    console.error('');
    
    const preview = await previewProposal({ projectPath, proposalId });
    console.log(JSON.stringify(preview, null, 2));
  } else if (command === 'apply') {
    console.error('Project:', projectPath);
    console.error('Applying proposal:', proposalId);
    console.error('');
    
    const result = await applyProposal({ projectPath, proposalId });
    console.log(JSON.stringify(result, null, 2));
  }
}

export default { 
  analyze, 
  previewProposal, 
  applyProposal, 
  applyAllProposals,
  getMemoryStatus, 
  getMemoryConfig, 
  getLLMStatus,
  runCLI 
};

export { startServer } from './mcp-server.js';
