// MCP Server - Main entry point for MCP protocol
import { scan } from '../scanner/index.js';
import { route } from '../router/index.js';
import { orchestrate, applyFallbackRules } from '../orchestrator/index.js';
import { memory } from '../memory/index.js';
import { fileEngine } from '../file-engine/index.js';
import { generateProposals, validateProposal } from '../proposals/index.js';
import { initLLM, isConfigured, chatWithSystem, loadConfig } from '../llm/index.js';
import { getSystemPrompt, getUserPrompt, validateBudget, TOKEN_BUDGET, OUTPUT_FORMAT } from '../llm/prompts.js';
import { configure as configureAgentsBridge } from '../agents-bridge.js';
import telemetry from '../telemetry/index.js';
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

// Initialize agents-bridge with config
try {
  const __filename = fileURLToPath(import.meta.url);
  const __dirname = path.dirname(__filename);
  const configPath = path.join(__dirname, '../../config/default.json');
  if (fs.existsSync(configPath)) {
    const cfg = JSON.parse(fs.readFileSync(configPath, 'utf-8'));
    if (cfg.agents_knowledge_base) {
      configureAgentsBridge(cfg.agents_knowledge_base);
      console.error('[MCP] Agents knowledge base configured:', cfg.agents_knowledge_base);
    }
  }
} catch (e) {
  console.error('[MCP] Warning: Could not configure agents-bridge:', e.message);
}

/**
 * In-memory store for pending proposals
 * In production, this should be persisted
 */
const pendingProposals = new Map();

// Valid agents whitelist
const VALID_AGENTS = ['frontend', 'backend', 'security', 'seo', 'test', 'code'];

/**
 * Validates if an agent is in the whitelist
 */
function isValidAgent(agentId) {
  return VALID_AGENTS.includes(agentId.toLowerCase());
}

/**
 * Emit telemetry event
 */
function emitTelemetry(eventName, data) {
  const event = {
    timestamp: new Date().toISOString(),
    event: eventName,
    ...data
  };
  console.error('[TELEMETRY]', JSON.stringify(event));
}

/**
 * Generate proposals using LLM (when available)
 * OPTIMIZED: Minimal tokens, compressed metadata
 * @param {string} projectPath
 * @param {string} userIntent
 * @param {Object} metadata
 * @param {string} [agentRules] - Optional agent rules from agents-bridge
 */
async function generateLLMProposals(projectPath, userIntent, metadata, agentRules) {
  if (!isConfigured()) {
    return null; // Fall back to deterministic
  }
  
  // OPTIMIZED: Compact prompts — inject agent rules if available
  const systemPrompt = getSystemPrompt(metadata, agentRules || '');
  const userPrompt = getUserPrompt(userIntent, metadata);
  
  // Token budget validation
  const budget = validateBudget(systemPrompt, userPrompt);
  if (!budget.valid) {
    console.error('[MCP] Token budget exceeded:', budget);
  }
  
  try {
    const response = await chatWithSystem(systemPrompt, userPrompt, {
      maxTokens: TOKEN_BUDGET.MAX_RESPONSE_TOKENS
    });
    
    if (!response.success) {
      console.error('[MCP] LLM error:', response.error);
      return null;
    }
    
    // Parse JSON response
    let changes;
    try {
      changes = JSON.parse(response.content);
    } catch {
      // Try to extract JSON from response
      const jsonMatch = response.content.match(/\[[\s\S]*\]/);
      if (jsonMatch) {
        changes = JSON.parse(jsonMatch[0]);
      } else {
        return null;
      }
    }
    
    // Handle compact format: {f, t, c} -> {file, type, content}
    const normalizedChanges = changes.map((change, i) => ({
      file: change.f || change.file,
      type: change.t || change.type,
      content: change.c || change.content
    })).filter(c => c.file && c.type);
    
    // Convert to proposals format
    return normalizedChanges.map((change, i) => ({
      id: 'llm-' + Date.now() + '-' + i,
      agent: 'llm',
      description: 'Generated by LLM: ' + change.file,
      change: {
        type: change.type === 'c' ? 'create' : change.type === 'u' ? 'update' : change.type === 'd' ? 'delete' : change.type,
        file: change.file,
        content: change.content || ''
      },
      originalContent: '',
      risks: ['LLM-generated code - review before applying'],
      llm: true,
      tokenUsage: budget.total
    }));
  } catch (e) {
    console.error('[MCP] LLM parsing error:', e.message);
    return null;
  }
}

/**
 * Main analyze function - MCP interface
 * Now includes proposal generation
 * @param {Object} params 
 * @param {string} params.projectPath
 * @param {string} params.userIntent
 * @param {boolean} params.generateProposals - Whether to generate proposals (default: true)
 * @param {string} params.forceAgent - Force a specific agent (bypasses router)
 * @returns {Object} MCPOutput
 */
export async function analyze({ projectPath, userIntent, generateProposals: doGenerateProposals = true, forceAgent = null }) {
  const promptId = 'prompt-' + Date.now();
  let forcedByUser = false;
  const startTime = Date.now();
  
  // Emit prompt received event
  telemetry.incPromptReceived();
  
  try {
    // Step 1: Scan project
    console.error('[MCP] Scanning project:', projectPath);
    const metadata = scan(projectPath);
    console.error('[MCP] Metadata:', JSON.stringify(metadata));
    
    // Step 2: Route to agents (or force agent)
    let plan, reason;
    
    if (forceAgent && isValidAgent(forceAgent)) {
      // Bypass router - use forced agent
      const agentId = forceAgent.toLowerCase();
      plan = [{ agentId, config: {} }];
      reason = `Forced by user: ${agentId}`;
      forcedByUser = true;
      
      // Emit override telemetry event
      emitTelemetry('override_invoked', {
        prompt_id: promptId,
        agent: agentId,
        user_intent: userIntent,
        forced: true
      });
      
      console.error('[MCP] Override: forcing agent:', agentId);
    } else if (forceAgent && !isValidAgent(forceAgent)) {
      // Invalid agent specified
      console.error('[MCP] Warning: Invalid agent specified:', forceAgent, '- falling back to router');
    }
    
    // Use router if no forced agent
    let agentsContext = null;
    if (!forceAgent || !isValidAgent(forceAgent)) {
      console.error('[MCP] Routing agents for intent:', userIntent);
      const routeResult = await route({ metadata, userIntent, projectPath });
      plan = routeResult.agents;
      reason = routeResult.reason;
      agentsContext = routeResult.agentsContext || null;
      
      // Apply fallback rules and scoring
      const scoringResult = applyFallbackRules({
        promptId,
        userIntent,
        keywordsScore: routeResult.scores?.keywords || 0.5,
        profileMatchScore: routeResult.scores?.profile || 0.5,
        historicalSuccessScore: 0.5,
        complexityEstimate: routeResult.scores?.complexity || 0.3
      });
      
      // Emit route decision and score telemetry
      telemetry.incRouteDecision(scoringResult.route, plan[0]?.agentId || 'unknown');
      telemetry.setScore(plan[0]?.agentId || 'unknown', scoringResult.score);
      
      // Emit fallback if needed
      if (scoringResult.route === 'fallback_llm') {
        telemetry.incFallbackInvoked('low_score');
      }
      
      console.error('[MCP] Routing score:', scoringResult.score, '- route:', scoringResult.route);
    }
    
    console.error('[MCP] Selected agents:', plan.map(a => a.agentId).join(', '));
    console.error('[MCP] Reason:', reason);
    
    // Emit agents activated telemetry
    for (const agent of plan) {
      telemetry.incAgentsActivated(agent.agentId);
    }
    
    // Step 3: Orchestrate execution (pass agents context from bridge)
    console.error('[MCP] Executing agents...');
    if (agentsContext?.matched) {
      console.error(`[MCP] Agents bridge: project=${agentsContext.projectId}, rules=${agentsContext.mdFiles?.length || 0} files`);
    }
    const { results, summary } = await orchestrate({
      projectPath,
      metadata,
      plan,
      userIntent,
      agentsContext
    });
    
    // Step 4: Collect diagnostics and changes
    const allDiagnostics = results.flatMap(r => r.diagnostics || []);
    const allChanges = results.flatMap(r => r.changes || []);
    
    // Step 5: Generate proposals if requested
    let proposals = [];
    if (doGenerateProposals) {
      console.error('[MCP] Generating proposals...');
      
      // Try LLM first if configured — pass agent rules for context-aware generation
      if (isConfigured()) {
        console.error('[MCP] Using LLM for intelligent proposals...');
        const agentRules = agentsContext?.context || '';
        const llmProposals = await generateLLMProposals(projectPath, userIntent, metadata, agentRules);
        if (llmProposals && llmProposals.length > 0) {
          proposals = llmProposals;
        }
      }
      
      // Fall back to deterministic if no LLM or no results
      if (proposals.length === 0) {
        console.error('[MCP] Using deterministic proposals...');
        const proposalResult = await generateProposals(projectPath, userIntent, metadata);
        proposals = proposalResult.proposals || [];
      }
      
      // Add forced_by_user metadata if agent was forced
      if (forcedByUser) {
        proposals = proposals.map(p => ({
          ...p,
          forced_by_user: true
        }));
      }
      
      // Store proposals for later apply
      if (proposals.length > 0) {
        const proposalKey = projectPath + ':' + Date.now();
        pendingProposals.set(proposalKey, { projectPath, proposals });
      }
    }
    
    // Step 6: Save to memory
    const agentIds = plan.map(p => p.agentId);
    const success = results.some(r => r.success);
    const runSummary = results.map(r => r.summary).join('; ');
    
    memory.saveRun(
      projectPath,
      agentIds,
      userIntent,
      success,
      runSummary
    );
    
    const memoryReference = memory.getReference(projectPath);
    
    // Emit latency telemetry
    const totalLatency = Date.now() - startTime;
    telemetry.recordLatency('analyze_total', totalLatency);
    
    // Estimate tokens saved (deterministic mode = no LLM tokens)
    if (!isConfigured()) {
      telemetry.incTokensSavedEstimate(150); // ~150 tokens saved by not calling LLM
    }
    
    // Return structured response
    return {
      summary: summary + ' ' + runSummary,
      diagnostics: allDiagnostics,
      changes: allChanges,
      proposals: proposals.map(p => ({
        id: p.id,
        agent: p.agent,
        description: p.description,
        file: p.change?.file,
        type: p.change?.type,
        diff: p.diff?.diff || '',
        additions: p.diff?.additions || 0,
        deletions: p.diff?.deletions || 0,
        risks: p.risks || []
      })),
      memoryReference
    };
    
  } catch (error) {
    console.error('[MCP] Error:', error.message);
    return {
      summary: 'Error: ' + error.message,
      diagnostics: [{
        severity: 'error',
        message: error.message,
        file: '',
        line: 0
      }],
      changes: [],
      proposals: [],
      memoryReference: ''
    };
  }
}

/**
 * Preview a proposal (dry-run)
 */
export async function previewProposal({ projectPath, proposalId }) {
  // Find proposal in pending
  let foundProposal = null;
  let proposalKey = null;
  
  for (const [key, data] of pendingProposals) {
    if (data.projectPath === projectPath) {
      foundProposal = data.proposals.find(p => p.id === proposalId);
      proposalKey = key;
      break;
    }
  }
  
  if (!foundProposal) {
    return {
      success: false,
      error: 'Proposal not found. Run analyze first to generate proposals.'
    };
  }
  
  // Validate
  const validation = validateProposal(foundProposal);
  if (!validation.valid) {
    return {
      success: false,
      errors: validation.errors,
      warnings: validation.warnings
    };
  }
  
  // Dry-run apply
  const preview = fileEngine.previewChanges(projectPath, [foundProposal.change]);
  
  return {
    success: true,
    proposal: {
      id: foundProposal.id,
      description: foundProposal.description,
      file: foundProposal.change?.file,
      type: foundProposal.change?.type,
      diff: foundProposal.diff?.diff || ''
    },
    preview: preview[0],
    validation: {
      valid: true,
      warnings: validation.warnings
    }
  };
}

/**
 * Apply a proposal
 */
export async function applyProposal({ projectPath, proposalId, backup = true }) {
  const startTime = Date.now();
  
  // Emit apply attempt
  if (telemetry) {
    telemetry.emit('apply_attempt', { proposalId, hasBackup: backup });
  }
  
  // Find proposal
  let foundProposal = null;
  
  for (const [key, data] of pendingProposals) {
    if (data.projectPath === projectPath) {
      foundProposal = data.proposals.find(p => p.id === proposalId);
      break;
    }
  }
  
  if (!foundProposal) {
    return {
      success: false,
      error: 'Proposal not found. Run analyze first to generate proposals.'
    };
  }
  
  // Validate first
  const validation = validateProposal(foundProposal);
  if (!validation.valid) {
    return {
      success: false,
      errors: validation.errors,
      warnings: validation.warnings
    };
  }
  
  // Apply with backup
  const applyFn = backup ? fileEngine.applyWithBackup : fileEngine.applyWithoutBackup;
  const result = applyFn(projectPath, [foundProposal.change]);
  
  // Calculate latency
  const latencyMs = Date.now() - startTime;
  const success = result[0]?.success || false;
  
  // Emit apply result
  if (telemetry) {
    telemetry.emit('apply_result', { 
      proposalId, 
      success, 
      latencyMs,
      hasBackup: backup
    });
  }
  
  return {
    success,
    applied: result[0]?.success ? [{
      file: foundProposal.change?.file,
      action: result[0]?.action,
      backupPath: result[0]?.backupPath || ''
    }] : [],
    errors: result[0]?.success ? [] : [result[0]?.error],
    warnings: validation.warnings
  };
}

/**
 * Apply all pending proposals
 */
export async function applyAllProposals({ projectPath, backup = true }) {
  const startTime = Date.now();
  
  let allProposals = [];
  
  for (const [key, data] of pendingProposals) {
    if (data.projectPath === projectPath) {
      allProposals = data.proposals;
      break;
    }
  }
  
  if (allProposals.length === 0) {
    return {
      success: false,
      error: 'No proposals found. Run analyze first.'
    };
  }
  
  // Emit apply attempt for each proposal
  if (telemetry) {
    allProposals.forEach(p => {
      telemetry.emit('apply_attempt', { proposalId: p.id, hasBackup: backup });
    });
  }
  
  // Apply all
  const applyFn = backup ? fileEngine.applyWithBackup : fileEngine.applyWithoutBackup;
  const result = applyFn(projectPath, allProposals.map(p => p.change));
  
  // Calculate latency
  const latencyMs = Date.now() - startTime;
  const allSuccess = result.every(r => r.success);
  
  // Emit apply result for each proposal
  if (telemetry) {
    result.forEach((r, i) => {
      telemetry.emit('apply_result', { 
        proposalId: allProposals[i]?.id, 
        success: r.success, 
        latencyMs: Math.round(latencyMs / result.length),
        hasBackup: backup
      });
    });
  }
  
  return {
    success: allSuccess,
    applied: result.map((r, i) => ({
      file: allProposals[i].change?.file,
      action: r.action,
      backupPath: r.backupPath || ''
    })),
    errors: result.filter(r => !r.success).map(r => r.error)
  };
}

/**
 * Get memory status for a project
 */
export async function getMemoryStatus({ projectPath }) {
  try {
    return memory.getStatus(projectPath);
  } catch (error) {
    return {
      error: error.message
    };
  }
}

/**
 * Get memory configuration
 */
export function getMemoryConfig() {
  return memory.getConfig();
}

/**
 * Get LLM status
 */
export function getLLMStatus() {
  if (!isConfigured()) {
    return {
      configured: false,
      message: 'LLM not configured. Set environment variables: LLM_PROVIDER, LLM_API_KEY, LLM_MODEL'
    };
  }
  
  const config = getConfig();
  return {
    configured: true,
    provider: config.provider,
    model: config.defaultModel,
    baseUrl: config.baseUrl || 'default'
  };
}

// CLI interface
export async function runCLI() {
  // Try to load LLM config from environment
  const llmConfig = loadConfig();
  if (llmConfig) {
    console.error('[CLI] LLM loaded:', llmConfig.provider, '-', llmConfig.defaultModel);
  }
  
  const args = process.argv.slice(2);
  let projectPath = '.';
  let userIntent = '';
  let command = 'analyze';
  let proposalId = '';
  let forceAgent = null;
  
  // Parse arguments
  for (let i = 0; i < args.length; i++) {
    if (args[i] === '--project' && args[i + 1]) {
      projectPath = args[i + 1];
      i++;
    } else if (args[i] === '--prompt' && args[i + 1]) {
      userIntent = args[i + 1];
      i++;
    } else if (args[i] === '--force-agent' && args[i + 1]) {
      forceAgent = args[i + 1];
      i++;
    } else if (args[i] === '--status') {
      command = 'status';
    } else if (args[i] === '--config') {
      command = 'config';
    } else if (args[i] === '--apply' && args[i + 1]) {
      command = 'apply';
      proposalId = args[i + 1];
      i++;
    } else if (args[i] === '--preview' && args[i + 1]) {
      command = 'preview';
      proposalId = args[i + 1];
      i++;
    } else if (args[i] === '--no-backup') {
      // Flag for apply commands
    }
  }
  
  if (command === 'analyze' && !userIntent) {
    console.log('Usage: node index.js --project <path> --prompt "<intent>"');
    console.log('       node index.js --project <path> --status');
    console.log('       node index.js --project <path> --config');
    console.log('       node index.js --project <path> --preview <proposal-id>');
    console.log('       node index.js --project <path> --apply <proposal-id>');
    console.log('       node index.js --project <path> --prompt "<intent>" --force-agent <agent>');
    console.log('       Valid agents: frontend, backend, security, seo, test, code');
    process.exit(1);
  }
  
  console.error('=== ai-core ===');
  
  if (command === 'analyze') {
    console.error('Project:', projectPath);
    console.error('Intent:', userIntent);
    if (forceAgent) {
      console.error('Force Agent:', forceAgent);
    }
    console.error('');
    
    const result = await analyze({ projectPath, userIntent, forceAgent });
    console.log(JSON.stringify(result, null, 2));
    
    if (result.proposals?.length > 0) {
      console.error('');
      console.error('Proposals generated. Use --preview or --apply to act on them.');
    }
  } else if (command === 'status') {
    console.error('Project:', projectPath);
    console.error('');
    
    const status = await getMemoryStatus({ projectPath });
    console.log(JSON.stringify(status, null, 2));
  } else if (command === 'config') {
    const config = getMemoryConfig();
    console.log(JSON.stringify(config, null, 2));
  } else if (command === 'preview') {
    console.error('Project:', projectPath);
    console.error('Proposal:', proposalId);
    console.error('');
    
    const preview = await previewProposal({ projectPath, proposalId });
    console.log(JSON.stringify(preview, null, 2));
  } else if (command === 'apply') {
    console.error('Project:', projectPath);
    console.error('Applying proposal:', proposalId);
    console.error('');
    
    const result = await applyProposal({ projectPath, proposalId });
    console.log(JSON.stringify(result, null, 2));
  }
}

export default { 
  analyze, 
  previewProposal, 
  applyProposal, 
  applyAllProposals,
  getMemoryStatus, 
  getMemoryConfig, 
  getLLMStatus,
  runCLI 
};

export { startServer } from './mcp-server.js';
